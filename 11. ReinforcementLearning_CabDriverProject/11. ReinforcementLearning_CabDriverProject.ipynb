{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3Q0JcN3z0R3"
   },
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhqvZwB-0DF1",
    "outputId": "deac2b5d-19af-4168-e14e-f6cf67f675cd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOdbWb6V0Dtq"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "py_file_location = \"/content/gdrive/MyDrive/Reinforcement Learning/RL Case Study/RL Project(Cab-Driver)-Code Structure\"\n",
    "sys.path.append(os.path.abspath(py_file_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "SIiNpQr1z0R5",
    "outputId": "14da5e08-3643-4c34-d7db-4416813845d3"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-aLSzycz0R6"
   },
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tblaHwb4z0R6"
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"/content/gdrive/MyDrive/Reinforcement Learning/RL Case Study/RL Project(Cab-Driver)-Code Structure/TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-J8o0kTz0R7"
   },
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open('/content/gdrive/MyDrive/Reinforcement Learning/RL Case Study/RL Project(Cab-Driver)-Code Structure/'+name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qc-10vo4z0R8"
   },
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK9naR3fz0R8"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.9\n",
    "        self.learning_rate =  0.01    \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = -0.0005\n",
    "        self.epsilon_min = 0.00001\n",
    "        \n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # Initialize the value of the states tracked\n",
    "        self.states_tracked = []\n",
    "        \n",
    "        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n",
    "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state,possible_actions_index, actions):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment       \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "          return random.choice(possible_actions_index)\n",
    "        else:\n",
    "          state = np.array(env.state_encod_arch1(state)).reshape(1, 36)     #reshaping to feed into neural net\n",
    "          q_value = self.model.predict(state)\n",
    "          possible_q_values = [q_value[0][i] for i in possible_actions_index]\n",
    "          return possible_actions_index[np.argmax(possible_q_values)]     \n",
    "\n",
    "    def append_sample(self, state, action_idx, reward, next_state, terminal_state):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "      self.memory.append((state, action_idx, reward, next_state, terminal_state))\n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size*3:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size)) \n",
    "            \n",
    "            actions, rewards, terminal_state = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "              state, action, reward, next_state, terminal_state_value = mini_batch[i]\n",
    "              update_input[i] = env.state_encod_arch1(state)\n",
    "              # Add action from memory\n",
    "              actions.append(action)\n",
    "              # Add reward from the memory\n",
    "              rewards.append(reward)\n",
    "              # Add next state s' to Q(s',a) from the memory\n",
    "              update_output[i] = env.state_encod_arch1(next_state)\n",
    "              terminal_state.append(terminal_state_value)\n",
    "\n",
    "            # Write your code from here\n",
    "            # 1. Predict the target from earlier model\n",
    "            pred = self.model.predict(update_input)\n",
    "            # 2. Get the target for the Q-network\n",
    "            pred_qvalue = self.model.predict(update_output)\n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "              if terminal_state[i]:\n",
    "                pred[i][actions[i]] = rewards[i]     #Only final reward is considered if episode has ended\n",
    "              else:\n",
    "                pred[i][actions[i]] = rewards[i] + self.discount_factor*np.max(pred_qvalue[i])   #If not terminal state, add discount factor to reward\n",
    "\n",
    "            # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, pred, batch_size=self.batch_size,epochs=1, verbose=0)\n",
    "        else:\n",
    "          pass\n",
    "\n",
    "    def save_tracking_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_tracked.append(q_value[0][2])\n",
    "        \n",
    "    def save_test_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_test.append(q_value[0][2])\n",
    "\n",
    "    def save(self, name):\n",
    "      with open('/content/gdrive/MyDrive/Reinforcement Learning/RL Case Study/RL Project(Cab-Driver)-Code Structure/'+name + '.pkl', 'wb') as file:\n",
    "        pickle.dump(self.model, file,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec-xv_Zsz0R9"
   },
   "outputs": [],
   "source": [
    "Episodes = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MDp5Ed8JiUJ",
    "outputId": "52c6680a-2c30-4433-b6f7-4d6924aaa898"
   },
   "outputs": [],
   "source": [
    "episode_length = 24*30 #30 days before which car has to be recharged\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()\n",
    "\n",
    "# Setting up state and action sizes.\n",
    "state_size = m+t+d         #Vector length\n",
    "action_size = len(action_space)\n",
    "\n",
    "# Calling agent class\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "# Rewards for state [0,0,0] being tracked.\n",
    "rewards_init_state = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNtm5nVEz0R9"
   },
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "eSEGWZZNz0R9",
    "outputId": "95fdb798-a9d0-4d93-c32d-bbe7dec1dcdf"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "scores_track = []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    terminal_state = False\n",
    "    score = 0\n",
    "    tracked_reward = False\n",
    "    \n",
    "\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    first_state = env.state_init\n",
    "    \n",
    "    run_time = 0\n",
    "\n",
    "    while not terminal_state:\n",
    "      possible_actions_index, actions = env.requests(state)\n",
    "      # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "      action = agent.get_action(state, possible_actions_index, actions)\n",
    "      # 2. Evaluate your reward and next state\n",
    "      reward = env.reward_func(state, env.action_space[action], Time_matrix)\n",
    "      next_state, next_time = env.next_state_func(state, env.action_space[action], Time_matrix)[::3]\n",
    "\n",
    "      run_time += next_time\n",
    "      if next_time > episode_length:\n",
    "        terminal_state = True\n",
    "      else:\n",
    "        # 3. Append the experience to the memory\n",
    "        agent.append_sample(state, action, reward, next_state, terminal_state)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model()\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        score += reward\n",
    "        state = next_state\n",
    "      \n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "\n",
    "    #Epsilon Decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "\n",
    "    # every 20 episodes:\n",
    "    if episode % 20 == 0:\n",
    "      print(\"Episode: {0}, Reward: {1}, Memory Length: {2}, Epsilon: {3}\".format(episode,score,len(agent.memory),agent.epsilon))\n",
    "    # every 10 episodes, store q-values of some prespecified state-action pairs:\n",
    "    if episode % 10 == 0:\n",
    "      agent.save_tracking_states()\n",
    "    \n",
    "    #Rewards per episode\n",
    "    scores_track.append(score)\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "      print(\"Saving Model {}\".format(episode))\n",
    "      agent.save(name=\"model_weights.pkl\")\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time Taken: '+str(round((end_time-start_time)/3600),2)+' hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0Fg8vKOz0R-"
   },
   "outputs": [],
   "source": [
    "agent.save(name=\"model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWoRw3IWSEA3"
   },
   "outputs": [],
   "source": [
    "agent.states_tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdMhdqyJSKz_"
   },
   "outputs": [],
   "source": [
    "state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nkw5MdK4z0R-"
   },
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5klQLySz0R-"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title('Q_value for state [0,0,0]  action (0,2)')\n",
    "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
    "plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A17ScGFYSQmE"
   },
   "outputs": [],
   "source": [
    "score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obiTFb_v1HN8"
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltoDJtoiSROJ"
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Rewards per episode')\n",
    "xaxis = np.asarray(range(0, len(score_tracked_sample)))\n",
    "plt.plot(xaxis,np.asarray(score_tracked_sample))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-nHJXRa1K1c"
   },
   "outputs": [],
   "source": [
    "print(\"Average reward of last 100 episodes is {0}\".format(np.mean(rewards_per_episode[-100:]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxpIQPpAz0R-"
   },
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BdRDGGuz0R-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PERKy65pz0R-"
   },
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63e13l-Fz0R-",
    "outputId": "9eb94801-3831-47aa-cbe6-b61d8083c204"
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZidR2zdhkBz6"
   },
   "source": [
    "## **Note:** <br>\n",
    "I could not finish the training due to severe Internet and system issues, which is why I was not able to attach the model_weights.pkl file. Apart from this, I have done the maximum I was able to, within my current abilities. I apologise for the same and request you to take this into consideration. <br>\n",
    "Thank you."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "h3Q0JcN3z0R3",
    "4-aLSzycz0R6",
    "qc-10vo4z0R8",
    "XNtm5nVEz0R9"
   ],
   "machine_shape": "hm",
   "name": "DQN_Agent_Arch1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
